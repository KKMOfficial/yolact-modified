{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnl4v6v3Pjul"
      },
      "source": [
        "#### Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMaIBvqp5hSB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install roboflow\n",
        "import zipfile\n",
        "\n",
        "import os, requests\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"yX1nJyVNr91CZv1vcBAf\")\n",
        "project = rf.workspace(\"minemy\").project(\"shoga-segmentation-combined-14030118\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# environment is going to be reset after this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQR5VlGq6Vr5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/KKMOfficial/yolact-modified.git\n",
        "%cd yolact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91v9ZlmlBn5I"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!activate yolact-env\n",
        "!pip install opencv-python\n",
        "!pip install pycocotools\n",
        "# environment is going to be reset after this\n",
        "!pip install torch torchvision torchaudio\n",
        "!conda install -c conda-forge tensorboard\n",
        "!pip install numpy==1.21.6\n",
        "!pip install matplotlib==3.4.0\n",
        "!pip install ipykernel\n",
        "!pip install wget\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install numpy==1.21.6"
      ],
      "metadata": {
        "id": "Z_gDK6u-gGc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvVeMcyruLBn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title source code modifications\n",
        "# line 131\n",
        "# shoga_dataset = dataset_base.copy({\n",
        "#   'name': 'Shoga_Dataset',\n",
        "#   'train_images':'/content/shoga-segmentation-combined-14030118-2/train',\n",
        "#   'train_info':'/content/shoga-segmentation-combined-14030118-2/train/_annotations.coco.json',\n",
        "#   'valid_images': '/content/shoga-segmentation-combined-14030118-2/valid',\n",
        "#   'valid_info':   '/content/shoga-segmentation-combined-14030118-2/valid/_annotations.coco.json',\n",
        "#   'class_names': ('bottle'),\n",
        "#   'label_map': { 1:  1 },\n",
        "# })\n",
        "# line 664\n",
        "# yolact_resnet101_bottle_config = coco_base_config.copy({\n",
        "#   'name': \"bottle_detection\",\n",
        "#   'dataset': shoga_dataset,\n",
        "#   'num_classes': len(shoga_dataset.class_names) + 1,\n",
        "# })\n",
        "# make only one class bottle in the dataset file\n",
        "# backbone.py : 143: state_dict = torch.load(path, map_location=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dzjT-qnGlob"
      },
      "outputs": [],
      "source": [
        "#@title downlaod backbone networks\n",
        "import os, requests, zipfile\n",
        "import wget\n",
        "\n",
        "def download_file(url, dest_dir, name):\n",
        "  os.makedirs(dest_dir, exist_ok=True)\n",
        "  wget.download(url,out = f\"{dest_dir}{name}\")\n",
        "\n",
        "\n",
        "\n",
        "darknet_url = 'https://drive.usercontent.google.com/download?id=1tvqFPd4bJtakOlmn-uIA492g2qurRChj&export=download&authuser=0&confirm=t&uuid=ab9c794e-700e-4d43-9dbf-a6a188005cad&at=APZUnTXB4dxsThsKb_eEtGNW_j1F%3A1721465302564'\n",
        "resnet_url = \"https://drive.usercontent.google.com/download?id=17Y431j4sagFpSReuPNoFcj9h7azDTZFf&export=download&authuser=0&confirm=t&uuid=91c570f0-0155-418c-8149-f68579cb7dba&at=APZUnTU7DDgtr1BiDTit8v7e1Wxn%3A1721478488916\"\n",
        "resnet50_url = \"https://drive.usercontent.google.com/download?id=1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn&export=download&authuser=0\"\n",
        "dataset_url = \"https://drive.usercontent.google.com/download?id=1PcJ8uwd6OGBG3w97zyH5KoxPeaSUd7k5&export=download&authuser=0&confirm=t&uuid=ce9e234a-2d81-4c33-b5f2-dc157128e6c7&at=APZUnTXbYqAFBbcNVZ6KlnBtVIY8:1721600907146\"\n",
        "dest_dir = '/content/yolact-modified/weights/'\n",
        "# file name : resnet101_reducedfc.pth\n",
        "# file name 2 : darknet53.pth\n",
        "# resnet50 : resnet50-19c8e357.pth\n",
        "\n",
        "download_file(resnet50_url, dest_dir, \"resnet50-19c8e357.pth\")\n",
        "download_file(resnet_url, dest_dir, \"resnet101_reducedfc.pth\")\n",
        "download_file(darknet_url, dest_dir, \"darknet53.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rstrHAORPjuq"
      },
      "source": [
        "#### Train Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs --samples_per_plugin \"images=10000\""
      ],
      "metadata": {
        "id": "HE5K3NvAh5z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yZ2_p4c3E2ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cefc8e4-1a1c-416e-d2da-69e2ccca8e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolact-modified\n",
            "\n",
            "EnvironmentNameNotFound: Could not find conda environment: yolact-env\n",
            "You can list all discoverable environments with `conda info --envs`.\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:314: UserWarning: 'pred_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:314: UserWarning: 'downsample_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:314: UserWarning: 'lat_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "Network Structure\n",
            "Yolact(\n",
            "  (backbone): ResNetBackbone(\n",
            "    (layers): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (proto_net): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): InterpolateModule()\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (fpn): FPN(\n",
            "    (lat_layers): RecursiveScriptModule(\n",
            "      original_name=ModuleList\n",
            "      (0): RecursiveScriptModule(original_name=Conv2d)\n",
            "      (1): RecursiveScriptModule(original_name=Conv2d)\n",
            "      (2): RecursiveScriptModule(original_name=Conv2d)\n",
            "    )\n",
            "    (pred_layers): RecursiveScriptModule(\n",
            "      original_name=ModuleList\n",
            "      (0): RecursiveScriptModule(original_name=Conv2d)\n",
            "      (1): RecursiveScriptModule(original_name=Conv2d)\n",
            "      (2): RecursiveScriptModule(original_name=Conv2d)\n",
            "    )\n",
            "    (downsample_layers): RecursiveScriptModule(\n",
            "      original_name=ModuleList\n",
            "      (0): RecursiveScriptModule(original_name=Conv2d)\n",
            "      (1): RecursiveScriptModule(original_name=Conv2d)\n",
            "    )\n",
            "  )\n",
            "  (prediction_layers): ModuleList(\n",
            "    (0): PredictionModule(\n",
            "      (upfeature): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bbox_layer): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conf_layer): Conv2d(256, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_layer): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (1-4): 4 x PredictionModule()\n",
            "  )\n",
            "  (semantic_seg_conv): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "Initializing weights...\n",
            "/content/yolact-modified/backbone.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=\"cuda:0\")\n",
            "Optimizer Object\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            "Learning Rate=0.001\n",
            "Momentum=0.9\n",
            "Weight Decay=0.0005\n",
            "Criterion Informatin\n",
            "MultiBoxLoss()\n",
            "/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Dataloader Informaton\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f23e7b8efe0>\n",
            "Begin training!\n",
            "Number Of Epochs=27587\n",
            "\n",
            "/content/yolact-modified/utils/augmentations.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  mode = random.choice(self.sample_options)\n",
            "/content/yolact-modified/utils/augmentations.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  mode = random.choice(self.sample_options)\n",
            "/content/yolact-modified/utils/augmentations.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  mode = random.choice(self.sample_options)\n",
            "/content/yolact-modified/utils/augmentations.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  mode = random.choice(self.sample_options)\n",
            "Optimizer Informatin\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            ">>>>>>>>>>>>>><class 'list'>\n",
            ">>>>>>>>>>>>>>8\n",
            ">>>>>>>>>>>>>>3\n",
            "Stopping early. Saving network...\n"
          ]
        }
      ],
      "source": [
        "%cd /content/yolact-modified\n",
        "!rm -rf runs\n",
        "!activate yolact-env\n",
        "!python train.py --config=yolact_resnet50_bottle_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBeE9F4sPjur"
      },
      "source": [
        "#### Check Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55zcoQAnkO_B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "model_path = '/content/yolact/weights/yolact_plus_resnet50_cig_butts_5_1394_interrupt.pth'\n",
        "\n",
        "# Load the model from the .pth file\n",
        "model = torch.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qceew5B-l8-N",
        "outputId": "c53a755c-f5f0-4fbe-bcd9-ce1ed0f129ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backbone.layers.0.0.conv1.weight\n",
            "backbone.layers.0.0.bn1.weight\n",
            "backbone.layers.0.0.bn1.bias\n",
            "backbone.layers.0.0.bn1.running_mean\n",
            "backbone.layers.0.0.bn1.running_var\n",
            "backbone.layers.0.0.bn1.num_batches_tracked\n",
            "backbone.layers.0.0.conv2.weight\n",
            "backbone.layers.0.0.bn2.weight\n",
            "backbone.layers.0.0.bn2.bias\n",
            "backbone.layers.0.0.bn2.running_mean\n",
            "backbone.layers.0.0.bn2.running_var\n",
            "backbone.layers.0.0.bn2.num_batches_tracked\n",
            "backbone.layers.0.0.conv3.weight\n",
            "backbone.layers.0.0.bn3.weight\n",
            "backbone.layers.0.0.bn3.bias\n",
            "backbone.layers.0.0.bn3.running_mean\n",
            "backbone.layers.0.0.bn3.running_var\n",
            "backbone.layers.0.0.bn3.num_batches_tracked\n",
            "backbone.layers.0.0.downsample.0.weight\n",
            "backbone.layers.0.0.downsample.1.weight\n",
            "backbone.layers.0.0.downsample.1.bias\n",
            "backbone.layers.0.0.downsample.1.running_mean\n",
            "backbone.layers.0.0.downsample.1.running_var\n",
            "backbone.layers.0.0.downsample.1.num_batches_tracked\n",
            "backbone.layers.0.1.conv1.weight\n",
            "backbone.layers.0.1.bn1.weight\n",
            "backbone.layers.0.1.bn1.bias\n",
            "backbone.layers.0.1.bn1.running_mean\n",
            "backbone.layers.0.1.bn1.running_var\n",
            "backbone.layers.0.1.bn1.num_batches_tracked\n",
            "backbone.layers.0.1.conv2.weight\n",
            "backbone.layers.0.1.bn2.weight\n",
            "backbone.layers.0.1.bn2.bias\n",
            "backbone.layers.0.1.bn2.running_mean\n",
            "backbone.layers.0.1.bn2.running_var\n",
            "backbone.layers.0.1.bn2.num_batches_tracked\n",
            "backbone.layers.0.1.conv3.weight\n",
            "backbone.layers.0.1.bn3.weight\n",
            "backbone.layers.0.1.bn3.bias\n",
            "backbone.layers.0.1.bn3.running_mean\n",
            "backbone.layers.0.1.bn3.running_var\n",
            "backbone.layers.0.1.bn3.num_batches_tracked\n",
            "backbone.layers.0.2.conv1.weight\n",
            "backbone.layers.0.2.bn1.weight\n",
            "backbone.layers.0.2.bn1.bias\n",
            "backbone.layers.0.2.bn1.running_mean\n",
            "backbone.layers.0.2.bn1.running_var\n",
            "backbone.layers.0.2.bn1.num_batches_tracked\n",
            "backbone.layers.0.2.conv2.weight\n",
            "backbone.layers.0.2.bn2.weight\n",
            "backbone.layers.0.2.bn2.bias\n",
            "backbone.layers.0.2.bn2.running_mean\n",
            "backbone.layers.0.2.bn2.running_var\n",
            "backbone.layers.0.2.bn2.num_batches_tracked\n",
            "backbone.layers.0.2.conv3.weight\n",
            "backbone.layers.0.2.bn3.weight\n",
            "backbone.layers.0.2.bn3.bias\n",
            "backbone.layers.0.2.bn3.running_mean\n",
            "backbone.layers.0.2.bn3.running_var\n",
            "backbone.layers.0.2.bn3.num_batches_tracked\n",
            "backbone.layers.1.0.conv1.weight\n",
            "backbone.layers.1.0.bn1.weight\n",
            "backbone.layers.1.0.bn1.bias\n",
            "backbone.layers.1.0.bn1.running_mean\n",
            "backbone.layers.1.0.bn1.running_var\n",
            "backbone.layers.1.0.bn1.num_batches_tracked\n",
            "backbone.layers.1.0.conv2.weight\n",
            "backbone.layers.1.0.bn2.weight\n",
            "backbone.layers.1.0.bn2.bias\n",
            "backbone.layers.1.0.bn2.running_mean\n",
            "backbone.layers.1.0.bn2.running_var\n",
            "backbone.layers.1.0.bn2.num_batches_tracked\n",
            "backbone.layers.1.0.conv3.weight\n",
            "backbone.layers.1.0.bn3.weight\n",
            "backbone.layers.1.0.bn3.bias\n",
            "backbone.layers.1.0.bn3.running_mean\n",
            "backbone.layers.1.0.bn3.running_var\n",
            "backbone.layers.1.0.bn3.num_batches_tracked\n",
            "backbone.layers.1.0.downsample.0.weight\n",
            "backbone.layers.1.0.downsample.1.weight\n",
            "backbone.layers.1.0.downsample.1.bias\n",
            "backbone.layers.1.0.downsample.1.running_mean\n",
            "backbone.layers.1.0.downsample.1.running_var\n",
            "backbone.layers.1.0.downsample.1.num_batches_tracked\n",
            "backbone.layers.1.1.conv1.weight\n",
            "backbone.layers.1.1.bn1.weight\n",
            "backbone.layers.1.1.bn1.bias\n",
            "backbone.layers.1.1.bn1.running_mean\n",
            "backbone.layers.1.1.bn1.running_var\n",
            "backbone.layers.1.1.bn1.num_batches_tracked\n",
            "backbone.layers.1.1.conv2.weight\n",
            "backbone.layers.1.1.bn2.weight\n",
            "backbone.layers.1.1.bn2.bias\n",
            "backbone.layers.1.1.bn2.running_mean\n",
            "backbone.layers.1.1.bn2.running_var\n",
            "backbone.layers.1.1.bn2.num_batches_tracked\n",
            "backbone.layers.1.1.conv3.weight\n",
            "backbone.layers.1.1.bn3.weight\n",
            "backbone.layers.1.1.bn3.bias\n",
            "backbone.layers.1.1.bn3.running_mean\n",
            "backbone.layers.1.1.bn3.running_var\n",
            "backbone.layers.1.1.bn3.num_batches_tracked\n",
            "backbone.layers.1.2.conv1.weight\n",
            "backbone.layers.1.2.bn1.weight\n",
            "backbone.layers.1.2.bn1.bias\n",
            "backbone.layers.1.2.bn1.running_mean\n",
            "backbone.layers.1.2.bn1.running_var\n",
            "backbone.layers.1.2.bn1.num_batches_tracked\n",
            "backbone.layers.1.2.conv2.weight\n",
            "backbone.layers.1.2.bn2.weight\n",
            "backbone.layers.1.2.bn2.bias\n",
            "backbone.layers.1.2.bn2.running_mean\n",
            "backbone.layers.1.2.bn2.running_var\n",
            "backbone.layers.1.2.bn2.num_batches_tracked\n",
            "backbone.layers.1.2.conv3.weight\n",
            "backbone.layers.1.2.bn3.weight\n",
            "backbone.layers.1.2.bn3.bias\n",
            "backbone.layers.1.2.bn3.running_mean\n",
            "backbone.layers.1.2.bn3.running_var\n",
            "backbone.layers.1.2.bn3.num_batches_tracked\n",
            "backbone.layers.1.3.conv1.weight\n",
            "backbone.layers.1.3.bn1.weight\n",
            "backbone.layers.1.3.bn1.bias\n",
            "backbone.layers.1.3.bn1.running_mean\n",
            "backbone.layers.1.3.bn1.running_var\n",
            "backbone.layers.1.3.bn1.num_batches_tracked\n",
            "backbone.layers.1.3.conv2.weight\n",
            "backbone.layers.1.3.bn2.weight\n",
            "backbone.layers.1.3.bn2.bias\n",
            "backbone.layers.1.3.bn2.running_mean\n",
            "backbone.layers.1.3.bn2.running_var\n",
            "backbone.layers.1.3.bn2.num_batches_tracked\n",
            "backbone.layers.1.3.conv3.weight\n",
            "backbone.layers.1.3.bn3.weight\n",
            "backbone.layers.1.3.bn3.bias\n",
            "backbone.layers.1.3.bn3.running_mean\n",
            "backbone.layers.1.3.bn3.running_var\n",
            "backbone.layers.1.3.bn3.num_batches_tracked\n",
            "backbone.layers.2.0.conv1.weight\n",
            "backbone.layers.2.0.bn1.weight\n",
            "backbone.layers.2.0.bn1.bias\n",
            "backbone.layers.2.0.bn1.running_mean\n",
            "backbone.layers.2.0.bn1.running_var\n",
            "backbone.layers.2.0.bn1.num_batches_tracked\n",
            "backbone.layers.2.0.conv2.weight\n",
            "backbone.layers.2.0.bn2.weight\n",
            "backbone.layers.2.0.bn2.bias\n",
            "backbone.layers.2.0.bn2.running_mean\n",
            "backbone.layers.2.0.bn2.running_var\n",
            "backbone.layers.2.0.bn2.num_batches_tracked\n",
            "backbone.layers.2.0.conv3.weight\n",
            "backbone.layers.2.0.bn3.weight\n",
            "backbone.layers.2.0.bn3.bias\n",
            "backbone.layers.2.0.bn3.running_mean\n",
            "backbone.layers.2.0.bn3.running_var\n",
            "backbone.layers.2.0.bn3.num_batches_tracked\n",
            "backbone.layers.2.0.downsample.0.weight\n",
            "backbone.layers.2.0.downsample.1.weight\n",
            "backbone.layers.2.0.downsample.1.bias\n",
            "backbone.layers.2.0.downsample.1.running_mean\n",
            "backbone.layers.2.0.downsample.1.running_var\n",
            "backbone.layers.2.0.downsample.1.num_batches_tracked\n",
            "backbone.layers.2.1.conv1.weight\n",
            "backbone.layers.2.1.bn1.weight\n",
            "backbone.layers.2.1.bn1.bias\n",
            "backbone.layers.2.1.bn1.running_mean\n",
            "backbone.layers.2.1.bn1.running_var\n",
            "backbone.layers.2.1.bn1.num_batches_tracked\n",
            "backbone.layers.2.1.conv2.weight\n",
            "backbone.layers.2.1.bn2.weight\n",
            "backbone.layers.2.1.bn2.bias\n",
            "backbone.layers.2.1.bn2.running_mean\n",
            "backbone.layers.2.1.bn2.running_var\n",
            "backbone.layers.2.1.bn2.num_batches_tracked\n",
            "backbone.layers.2.1.conv3.weight\n",
            "backbone.layers.2.1.bn3.weight\n",
            "backbone.layers.2.1.bn3.bias\n",
            "backbone.layers.2.1.bn3.running_mean\n",
            "backbone.layers.2.1.bn3.running_var\n",
            "backbone.layers.2.1.bn3.num_batches_tracked\n",
            "backbone.layers.2.2.conv1.weight\n",
            "backbone.layers.2.2.bn1.weight\n",
            "backbone.layers.2.2.bn1.bias\n",
            "backbone.layers.2.2.bn1.running_mean\n",
            "backbone.layers.2.2.bn1.running_var\n",
            "backbone.layers.2.2.bn1.num_batches_tracked\n",
            "backbone.layers.2.2.conv2.weight\n",
            "backbone.layers.2.2.bn2.weight\n",
            "backbone.layers.2.2.bn2.bias\n",
            "backbone.layers.2.2.bn2.running_mean\n",
            "backbone.layers.2.2.bn2.running_var\n",
            "backbone.layers.2.2.bn2.num_batches_tracked\n",
            "backbone.layers.2.2.conv3.weight\n",
            "backbone.layers.2.2.bn3.weight\n",
            "backbone.layers.2.2.bn3.bias\n",
            "backbone.layers.2.2.bn3.running_mean\n",
            "backbone.layers.2.2.bn3.running_var\n",
            "backbone.layers.2.2.bn3.num_batches_tracked\n",
            "backbone.layers.2.3.conv1.weight\n",
            "backbone.layers.2.3.bn1.weight\n",
            "backbone.layers.2.3.bn1.bias\n",
            "backbone.layers.2.3.bn1.running_mean\n",
            "backbone.layers.2.3.bn1.running_var\n",
            "backbone.layers.2.3.bn1.num_batches_tracked\n",
            "backbone.layers.2.3.conv2.weight\n",
            "backbone.layers.2.3.bn2.weight\n",
            "backbone.layers.2.3.bn2.bias\n",
            "backbone.layers.2.3.bn2.running_mean\n",
            "backbone.layers.2.3.bn2.running_var\n",
            "backbone.layers.2.3.bn2.num_batches_tracked\n",
            "backbone.layers.2.3.conv3.weight\n",
            "backbone.layers.2.3.bn3.weight\n",
            "backbone.layers.2.3.bn3.bias\n",
            "backbone.layers.2.3.bn3.running_mean\n",
            "backbone.layers.2.3.bn3.running_var\n",
            "backbone.layers.2.3.bn3.num_batches_tracked\n",
            "backbone.layers.2.4.conv1.weight\n",
            "backbone.layers.2.4.bn1.weight\n",
            "backbone.layers.2.4.bn1.bias\n",
            "backbone.layers.2.4.bn1.running_mean\n",
            "backbone.layers.2.4.bn1.running_var\n",
            "backbone.layers.2.4.bn1.num_batches_tracked\n",
            "backbone.layers.2.4.conv2.weight\n",
            "backbone.layers.2.4.bn2.weight\n",
            "backbone.layers.2.4.bn2.bias\n",
            "backbone.layers.2.4.bn2.running_mean\n",
            "backbone.layers.2.4.bn2.running_var\n",
            "backbone.layers.2.4.bn2.num_batches_tracked\n",
            "backbone.layers.2.4.conv3.weight\n",
            "backbone.layers.2.4.bn3.weight\n",
            "backbone.layers.2.4.bn3.bias\n",
            "backbone.layers.2.4.bn3.running_mean\n",
            "backbone.layers.2.4.bn3.running_var\n",
            "backbone.layers.2.4.bn3.num_batches_tracked\n",
            "backbone.layers.2.5.conv1.weight\n",
            "backbone.layers.2.5.bn1.weight\n",
            "backbone.layers.2.5.bn1.bias\n",
            "backbone.layers.2.5.bn1.running_mean\n",
            "backbone.layers.2.5.bn1.running_var\n",
            "backbone.layers.2.5.bn1.num_batches_tracked\n",
            "backbone.layers.2.5.conv2.weight\n",
            "backbone.layers.2.5.bn2.weight\n",
            "backbone.layers.2.5.bn2.bias\n",
            "backbone.layers.2.5.bn2.running_mean\n",
            "backbone.layers.2.5.bn2.running_var\n",
            "backbone.layers.2.5.bn2.num_batches_tracked\n",
            "backbone.layers.2.5.conv3.weight\n",
            "backbone.layers.2.5.bn3.weight\n",
            "backbone.layers.2.5.bn3.bias\n",
            "backbone.layers.2.5.bn3.running_mean\n",
            "backbone.layers.2.5.bn3.running_var\n",
            "backbone.layers.2.5.bn3.num_batches_tracked\n",
            "backbone.layers.3.0.conv1.weight\n",
            "backbone.layers.3.0.bn1.weight\n",
            "backbone.layers.3.0.bn1.bias\n",
            "backbone.layers.3.0.bn1.running_mean\n",
            "backbone.layers.3.0.bn1.running_var\n",
            "backbone.layers.3.0.bn1.num_batches_tracked\n",
            "backbone.layers.3.0.conv2.weight\n",
            "backbone.layers.3.0.bn2.weight\n",
            "backbone.layers.3.0.bn2.bias\n",
            "backbone.layers.3.0.bn2.running_mean\n",
            "backbone.layers.3.0.bn2.running_var\n",
            "backbone.layers.3.0.bn2.num_batches_tracked\n",
            "backbone.layers.3.0.conv3.weight\n",
            "backbone.layers.3.0.bn3.weight\n",
            "backbone.layers.3.0.bn3.bias\n",
            "backbone.layers.3.0.bn3.running_mean\n",
            "backbone.layers.3.0.bn3.running_var\n",
            "backbone.layers.3.0.bn3.num_batches_tracked\n",
            "backbone.layers.3.0.downsample.0.weight\n",
            "backbone.layers.3.0.downsample.1.weight\n",
            "backbone.layers.3.0.downsample.1.bias\n",
            "backbone.layers.3.0.downsample.1.running_mean\n",
            "backbone.layers.3.0.downsample.1.running_var\n",
            "backbone.layers.3.0.downsample.1.num_batches_tracked\n",
            "backbone.layers.3.1.conv1.weight\n",
            "backbone.layers.3.1.bn1.weight\n",
            "backbone.layers.3.1.bn1.bias\n",
            "backbone.layers.3.1.bn1.running_mean\n",
            "backbone.layers.3.1.bn1.running_var\n",
            "backbone.layers.3.1.bn1.num_batches_tracked\n",
            "backbone.layers.3.1.conv2.weight\n",
            "backbone.layers.3.1.bn2.weight\n",
            "backbone.layers.3.1.bn2.bias\n",
            "backbone.layers.3.1.bn2.running_mean\n",
            "backbone.layers.3.1.bn2.running_var\n",
            "backbone.layers.3.1.bn2.num_batches_tracked\n",
            "backbone.layers.3.1.conv3.weight\n",
            "backbone.layers.3.1.bn3.weight\n",
            "backbone.layers.3.1.bn3.bias\n",
            "backbone.layers.3.1.bn3.running_mean\n",
            "backbone.layers.3.1.bn3.running_var\n",
            "backbone.layers.3.1.bn3.num_batches_tracked\n",
            "backbone.layers.3.2.conv1.weight\n",
            "backbone.layers.3.2.bn1.weight\n",
            "backbone.layers.3.2.bn1.bias\n",
            "backbone.layers.3.2.bn1.running_mean\n",
            "backbone.layers.3.2.bn1.running_var\n",
            "backbone.layers.3.2.bn1.num_batches_tracked\n",
            "backbone.layers.3.2.conv2.weight\n",
            "backbone.layers.3.2.bn2.weight\n",
            "backbone.layers.3.2.bn2.bias\n",
            "backbone.layers.3.2.bn2.running_mean\n",
            "backbone.layers.3.2.bn2.running_var\n",
            "backbone.layers.3.2.bn2.num_batches_tracked\n",
            "backbone.layers.3.2.conv3.weight\n",
            "backbone.layers.3.2.bn3.weight\n",
            "backbone.layers.3.2.bn3.bias\n",
            "backbone.layers.3.2.bn3.running_mean\n",
            "backbone.layers.3.2.bn3.running_var\n",
            "backbone.layers.3.2.bn3.num_batches_tracked\n",
            "backbone.conv1.weight\n",
            "backbone.bn1.weight\n",
            "backbone.bn1.bias\n",
            "backbone.bn1.running_mean\n",
            "backbone.bn1.running_var\n",
            "backbone.bn1.num_batches_tracked\n",
            "proto_net.0.weight\n",
            "proto_net.0.bias\n",
            "proto_net.2.weight\n",
            "proto_net.2.bias\n",
            "proto_net.4.weight\n",
            "proto_net.4.bias\n",
            "proto_net.8.weight\n",
            "proto_net.8.bias\n",
            "proto_net.10.weight\n",
            "proto_net.10.bias\n",
            "fpn.lat_layers.0.weight\n",
            "fpn.lat_layers.0.bias\n",
            "fpn.lat_layers.1.weight\n",
            "fpn.lat_layers.1.bias\n",
            "fpn.lat_layers.2.weight\n",
            "fpn.lat_layers.2.bias\n",
            "fpn.pred_layers.0.weight\n",
            "fpn.pred_layers.0.bias\n",
            "fpn.pred_layers.1.weight\n",
            "fpn.pred_layers.1.bias\n",
            "fpn.pred_layers.2.weight\n",
            "fpn.pred_layers.2.bias\n",
            "fpn.downsample_layers.0.weight\n",
            "fpn.downsample_layers.0.bias\n",
            "fpn.downsample_layers.1.weight\n",
            "fpn.downsample_layers.1.bias\n",
            "prediction_layers.0.upfeature.0.weight\n",
            "prediction_layers.0.upfeature.0.bias\n",
            "prediction_layers.0.bbox_layer.weight\n",
            "prediction_layers.0.bbox_layer.bias\n",
            "prediction_layers.0.conf_layer.weight\n",
            "prediction_layers.0.conf_layer.bias\n",
            "prediction_layers.0.mask_layer.weight\n",
            "prediction_layers.0.mask_layer.bias\n",
            "semantic_seg_conv.weight\n",
            "semantic_seg_conv.bias\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, dict) and 'state_dict' in model:\n",
        "    # The model is wrapped in an OrderedDict, so we need to access the 'state_dict'\n",
        "    model = model['state_dict']\n",
        "\n",
        "for name, _ in model.items():\n",
        "    print(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUI9HOnfmfsG",
        "outputId": "916f5846-3ed3-4ab0-d5db-03fd6ca988ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/torch/__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:313: UserWarning: 'pred_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:313: UserWarning: 'lat_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/site-packages/torch/jit/_recursive.py:313: UserWarning: 'downsample_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\n",
            "Loading model... Done.\n",
            "\n",
            "/content/folder1/cig_butts/real_test/0008.JPG -> output_images/0008.png\n",
            "/content/folder1/cig_butts/real_test/0004.JPG -> output_images/0004.png\n",
            "/content/folder1/cig_butts/real_test/0006.JPG -> output_images/0006.png\n",
            "/content/folder1/cig_butts/real_test/0000.JPG -> output_images/0000.png\n",
            "/content/folder1/cig_butts/real_test/0007.JPG -> output_images/0007.png\n",
            "/content/folder1/cig_butts/real_test/0003.JPG -> output_images/0003.png\n",
            "/content/folder1/cig_butts/real_test/0009.JPG -> output_images/0009.png\n",
            "/content/folder1/cig_butts/real_test/0005.JPG -> output_images/0005.png\n",
            "/content/folder1/cig_butts/real_test/0001.JPG -> output_images/0001.png\n",
            "/content/folder1/cig_butts/real_test/0002.JPG -> output_images/0002.png\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "!python eval.py --trained_model=/content/yolact/weights/yolact_plus_resnet50_cig_butts_5_1394_interrupt.pth --config=yolact_resnet50_cig_butts_config --score_threshold=0.3 --top_k=15 --images=/content/folder1/cig_butts/real_test:output_images"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "LBeE9F4sPjur"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}